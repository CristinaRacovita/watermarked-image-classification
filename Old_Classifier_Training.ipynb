{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4192f95a-1ec8-4d24-af66-a4f878ebb1c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kagglehub in ./.local/lib/python3.8/site-packages (0.2.9)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from kagglehub) (24.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kagglehub) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kagglehub) (4.66.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kagglehub) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->kagglehub) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->kagglehub) (2019.11.28)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae7bc1b-8bbb-483e-8a43-b2030c0ac992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8528b2-6bdc-42fc-b9db-be0323728183",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/tanlikesmath/the-oxfordiiit-pet-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.48G/1.48G [00:51<00:00, 31.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded at: /home/jovyan/.cache/kagglehub/datasets/tanlikesmath/the-oxfordiiit-pet-dataset/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "def load_oxford_pet_dataset():\n",
    "    return kagglehub.dataset_download(\"tanlikesmath/the-oxfordiiit-pet-dataset\")\n",
    "\n",
    "path_to_dataset = load_oxford_pet_dataset()\n",
    "print(f\"Dataset downloaded at: {path_to_dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e66cf7-afc0-47f1-825a-2fd9c348624d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_images_dataset(images_path):\n",
    "    image_files = [f for f in os.listdir(images_path) if f.lower()\n",
    "                   .endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    image_data = []\n",
    "\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(images_path, image_file)\n",
    "        img = Image.open(image_path)\n",
    "\n",
    "        image_data.append({\n",
    "            \"image_name\": image_file,\n",
    "            \"image_path\": image_path,\n",
    "            \"image_size\": img.size\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(image_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169c7caa-898a-4beb-b522-8890c8e51862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = \"/home/jovyan/.cache/kagglehub/datasets/tanlikesmath/the-oxfordiiit-pet-dataset/versions/1/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec24376f-6065-4944-beae-fa665ee1833b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image_path</th>\n",
       "      <th>image_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abyssinian_1.jpg</td>\n",
       "      <td>/home/jovyan/.cache/kagglehub/datasets/tanlike...</td>\n",
       "      <td>(600, 400)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abyssinian_10.jpg</td>\n",
       "      <td>/home/jovyan/.cache/kagglehub/datasets/tanlike...</td>\n",
       "      <td>(375, 500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abyssinian_100.jpg</td>\n",
       "      <td>/home/jovyan/.cache/kagglehub/datasets/tanlike...</td>\n",
       "      <td>(394, 500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abyssinian_101.jpg</td>\n",
       "      <td>/home/jovyan/.cache/kagglehub/datasets/tanlike...</td>\n",
       "      <td>(450, 313)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abyssinian_102.jpg</td>\n",
       "      <td>/home/jovyan/.cache/kagglehub/datasets/tanlike...</td>\n",
       "      <td>(500, 465)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_name                                         image_path  \\\n",
       "0    Abyssinian_1.jpg  /home/jovyan/.cache/kagglehub/datasets/tanlike...   \n",
       "1   Abyssinian_10.jpg  /home/jovyan/.cache/kagglehub/datasets/tanlike...   \n",
       "2  Abyssinian_100.jpg  /home/jovyan/.cache/kagglehub/datasets/tanlike...   \n",
       "3  Abyssinian_101.jpg  /home/jovyan/.cache/kagglehub/datasets/tanlike...   \n",
       "4  Abyssinian_102.jpg  /home/jovyan/.cache/kagglehub/datasets/tanlike...   \n",
       "\n",
       "   image_size  \n",
       "0  (600, 400)  \n",
       "1  (375, 500)  \n",
       "2  (394, 500)  \n",
       "3  (450, 313)  \n",
       "4  (500, 465)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oxford_pet_dataset = read_images_dataset(dataset_path)\n",
    "oxford_pet_dataset.head(5)  # Show the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2edd61a-31cb-4bb5-9051-745b5475e9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize_image(image_path, dim):\n",
    "    image = Image.open(image_path)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    new_size = (dim, dim)\n",
    "    resized_image = image.resize(new_size)\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d8ce8e6-88d5-45c5-923e-2b21dea083a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_resize_oxford_pet_dataset(original_dataset):\n",
    "    output_dir = \"resized_images\"\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for index, row in original_dataset.iterrows():\n",
    "        input_image_path = row['image_path']\n",
    "        resized_image = resize_image(input_image_path, 224)\n",
    "\n",
    "        output_image_path = os.path.join(output_dir, row['image_name'])\n",
    "        resized_image.save(output_image_path)\n",
    "\n",
    "    return output_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc1ae9b-0bb8-4345-80b6-93baf03ac381",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized images saved to: resized_images\n"
     ]
    }
   ],
   "source": [
    "resized_dataset_dir_path = save_resize_oxford_pet_dataset(oxford_pet_dataset)\n",
    "print(f\"Resized images saved to: {resized_dataset_dir_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "508a1997-d414-423a-a0bc-95838595bfaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def organize_images_by_class(dataset_df, output_dir=\"resized_images\"):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for _, row in dataset_df.iterrows():\n",
    "        class_name = row[\"image_name\"].split(\"_\")[0]  # Extract class name\n",
    "        class_dir = os.path.join(output_dir, class_name)\n",
    "\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "\n",
    "        # Move the image into the class folder\n",
    "        shutil.move(row[\"image_path\"], os.path.join(class_dir, row[\"image_name\"]))\n",
    "\n",
    "    print(f\"Images organized into class folders in: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e688ee2-294f-44da-a1e0-f134f7499588",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images organized into class folders in: resized_images\n"
     ]
    }
   ],
   "source": [
    "organize_images_by_class(oxford_pet_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ae6f9e3-ab97-4ba3-92cb-50e0f1ca7844",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abyssinian_1.jpg', 'Abyssinian_10.jpg', 'Abyssinian_100.jpg', 'Abyssinian_101.jpg', 'Abyssinian_102.jpg', 'Abyssinian_103.jpg', 'Abyssinian_104.jpg', 'Abyssinian_105.jpg', 'Abyssinian_106.jpg', 'Abyssinian_107.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"resized_images\")[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7621937-d9ce-46ee-98d3-e135fdd46d07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.12-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from timm) (1.13.1+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm) (0.14.1+cu116)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm) (6.0.1)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from huggingface_hub->timm)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub->timm) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub->timm) (24.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub->timm) (4.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (1.23.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (10.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub->timm) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface_hub->timm) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub->timm) (2019.11.28)\n",
      "Downloading timm-1.0.12-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.1/436.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: safetensors, filelock, huggingface_hub, timm\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed filelock-3.16.1 huggingface_hub-0.27.0 safetensors-0.4.5 timm-1.0.12\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d263135-5bc4-4776-a120-d247bacd5879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import timm  # To use the TinyViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79417d19-191b-49a4-b8ea-0ffdbeab0f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in the training dataset: 7390\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# Load the organized dataset\n",
    "train_dataset = datasets.ImageFolder(root=\"resized_images\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Total images in the training dataset: {len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "455c6391-ebdc-42d2-8ba3-63ba2fa8d32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up device for GPU/CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load TinyViT pretrained model\n",
    "model = timm.create_model(\"tiny_vit_5m_224\", pretrained=True, num_classes=len(train_dataset.classes))\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb6f465e-2a29-430f-9f61-7dcce0e4df31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 5912\n",
      "Validation set size: 1478\n"
     ]
    }
   ],
   "source": [
    "# Split dataset: 80% training, 20% validation\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create train and validation DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a73cdd8e-4215-481f-91f8-042e7fb9886d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "Train Loss: 2.5884, Train Acc: 57.48%\n",
      "Val Loss: 1.6433, Val Acc: 85.39%\n",
      "Epoch [2/5]\n",
      "Train Loss: 1.1390, Train Acc: 88.09%\n",
      "Val Loss: 0.7905, Val Acc: 89.45%\n",
      "Epoch [3/5]\n",
      "Train Loss: 0.5377, Train Acc: 93.81%\n",
      "Val Loss: 0.4743, Val Acc: 92.15%\n",
      "Epoch [4/5]\n",
      "Train Loss: 0.2820, Train Acc: 96.84%\n",
      "Val Loss: 0.3610, Val Acc: 92.63%\n",
      "Epoch [5/5]\n",
      "Train Loss: 0.1584, Train Acc: 98.58%\n",
      "Val Loss: 0.3237, Val Acc: 91.41%\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss, correct, total = 0, 0, 0\n",
    "        \n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {running_loss/len(train_loader):.4f}, Train Acc: {correct/total:.2%}\")\n",
    "        print(f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_correct/val_total:.2%}\")\n",
    "        \n",
    "        model.train()  # Switch back to train mode\n",
    "\n",
    "# Call the training function\n",
    "train(model, train_loader, val_loader, criterion, optimizer, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7112ef34-c935-4d87-b4c5-3c4711a66919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'tinyvit_trained_model.pth'\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"tinyvit_trained_model.pth\")\n",
    "print(\"Model saved as 'tinyvit_trained_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89bde1b-8293-4468-beca-aa75c4f60b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
