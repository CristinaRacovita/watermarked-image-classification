{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a21eda0-49a8-4449-8303-ea911df392c9",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (default values will be overwritten by Papermill)\n",
    "input_dir = \"default_input_dir\"\n",
    "output_dir = \"default_output_dir\"\n",
    "output_der = \"default_output_der\"\n",
    "input_model_name = \"default_input_model_name\"\n",
    "input_model_path = \"default_model_save_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b21999-3529-4541-af30-efe0305756a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_name = \"apple/mobilevit-xx-small\"\n",
    "model_name = \"tiny_vit_5m_224\"\n",
    "model_save_path = f\"models/tiny_vit_5m_224_2025-01-06_17-38.pth\"\n",
    "#model_save_path = f\"models/apple_mobilevit-xx-small_2025-01-06_16-59.pth\"\n",
    "\n",
    "#model_name = input_model_name\n",
    "#model_save_path = input_model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c146e07-ee1f-4aa9-aad9-673ca299b919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 21:16:02.681661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-26 21:16:02.866705: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-26 21:16:03.669520: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-01-26 21:16:03.669611: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2025-01-26 21:16:03.669616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "## Import Statements\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import os\n",
    "\n",
    "import timm  # To use the TinyViT model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# set constants\n",
    "#SEED = 19\n",
    "CLASSES_NO = 37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3221d39-904d-40cc-887c-6437b2e1e30c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Start With Classifying without watermarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50925429-2dd0-46c7-bd88-6451e2eaf29c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TinyViT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_145/121078527.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TinyVit(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (conv1): ConvNorm(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act): GELU(approximate='none')\n",
       "    (conv2): ConvNorm(\n",
       "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (stages): Sequential(\n",
       "    (0): ConvLayer(\n",
       "      (blocks): Sequential(\n",
       "        (0): MBConv(\n",
       "          (conv1): ConvNorm(\n",
       "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): ConvNorm(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act2): GELU(approximate='none')\n",
       "          (conv3): ConvNorm(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act3): GELU(approximate='none')\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): MBConv(\n",
       "          (conv1): ConvNorm(\n",
       "            (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act1): GELU(approximate='none')\n",
       "          (conv2): ConvNorm(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "            (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act2): GELU(approximate='none')\n",
       "          (conv3): ConvNorm(\n",
       "            (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (act3): GELU(approximate='none')\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): TinyVitStage(\n",
       "      dim=128, depth=2\n",
       "      (downsample): PatchMerging(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act1): GELU(approximate='none')\n",
       "        (conv2): ConvNorm(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): GELU(approximate='none')\n",
       "        (conv3): ConvNorm(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): TinyVitBlock(\n",
       "          dim=128, num_heads=4, window_size=7, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TinyVitBlock(\n",
       "          dim=128, num_heads=4, window_size=7, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): TinyVitStage(\n",
       "      dim=160, depth=6\n",
       "      (downsample): PatchMerging(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv2d(128, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act1): GELU(approximate='none')\n",
       "        (conv2): ConvNorm(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): GELU(approximate='none')\n",
       "        (conv3): ConvNorm(\n",
       "          (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): TinyVitBlock(\n",
       "          dim=160, num_heads=5, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
       "            (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TinyVitBlock(\n",
       "          dim=160, num_heads=5, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
       "            (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (2): TinyVitBlock(\n",
       "          dim=160, num_heads=5, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
       "            (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (3): TinyVitBlock(\n",
       "          dim=160, num_heads=5, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
       "            (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (4): TinyVitBlock(\n",
       "          dim=160, num_heads=5, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
       "            (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (5): TinyVitBlock(\n",
       "          dim=160, num_heads=5, window_size=14, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
       "            (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): TinyVitStage(\n",
       "      dim=320, depth=2\n",
       "      (downsample): PatchMerging(\n",
       "        (conv1): ConvNorm(\n",
       "          (conv): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act1): GELU(approximate='none')\n",
       "        (conv2): ConvNorm(\n",
       "          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=320, bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (act2): GELU(approximate='none')\n",
       "        (conv3): ConvNorm(\n",
       "          (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (blocks): Sequential(\n",
       "        (0): TinyVitBlock(\n",
       "          dim=320, num_heads=10, window_size=7, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): TinyVitBlock(\n",
       "          dim=320, num_heads=10, window_size=7, mlp_ratio=4.0\n",
       "          (attn): Attention(\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
       "            (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): NormMlp(\n",
       "            (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path2): Identity()\n",
       "          (local_conv): ConvNorm(\n",
       "            (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (head): NormMlpClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (norm): LayerNorm2d((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (pre_logits): Identity()\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc): Linear(in_features=320, out_features=37, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 37  # Update this if you have a different number of classes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if model_name == \"apple/mobilevit-xx-small\":\n",
    "    print(\"Using MobileViT\")\n",
    "    # Load the processor\n",
    "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "    # Load the pretrained model\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    # Update the classifier to match the dataset classes\n",
    "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "\n",
    "elif model_name == \"tiny_vit_5m_224\":\n",
    "    print(\"Using TinyViT\")\n",
    "    # Import timm for TinyViT model loading\n",
    "    import timm\n",
    "\n",
    "    # Load the pretrained TinyViT model\n",
    "    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Unsupported model type. Please use MobileViT or TinyViT.\")\n",
    "\n",
    "# Load fine-tuned weights\n",
    "try:\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Model weights not found at {model_save_path}\")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c463faf5-923d-4200-8ac9-84a6fd389578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize image to 224x224\n",
    "    transforms.ToTensor(),          # Convert image to tensor\n",
    "    transforms.Normalize(           # Normalize with ImageNet statistics\n",
    "        #mean=[0.485, 0.456, 0.406],\n",
    "        mean = [0.4784, 0.4460, 0.39609],\n",
    "        #std=[0.229, 0.224, 0.225]\n",
    "        std = [0.2245, 0.2216, 0.2227] \n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea61969-fde7-448d-b4dd-93ff2c418b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_images_from_folder(folder_path, model, transform, device):\n",
    "    predictions = []\n",
    "\n",
    "    for image_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "        # Skip non-image files\n",
    "        if not image_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        # Open and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor).logits\n",
    "            predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "            predictions.append({\"image_name\": image_name, \"predicted_class\": predicted_class})\n",
    "\n",
    "    # Return predictions as a DataFrame\n",
    "    return pd.DataFrame(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c22e608-8b78-4bf8-834d-219bba5ea747",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_images_from_folder(folder_path, model, transform, device):\n",
    "    predictions = []\n",
    "\n",
    "    for image_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "        # Skip non-image files\n",
    "        if not image_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        # Open and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)  # Get raw model output\n",
    "            if isinstance(outputs, torch.Tensor):  # Check if it's a PyTorch model\n",
    "                predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "            else:  # Assume it's a Hugging Face model output\n",
    "                predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "            predictions.append({\"image_name\": image_name, \"predicted_class\": predicted_class})\n",
    "\n",
    "    # Return predictions as a DataFrame\n",
    "    return pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795e3850-ba50-4b3e-888a-9ad4ff6ef543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the folders\n",
    "test_folder = \"./data/resized_images/test\"\n",
    "#trainval_folder = \"./data/resized_images/trainval\"\n",
    "\n",
    "# Classify images in test folder\n",
    "test_predictions = classify_images_from_folder(test_folder, model, transform, device)\n",
    "\n",
    "# Classify images in trainval folder\n",
    "#trainval_predictions = classify_images_from_folder(trainval_folder, model, transform, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36bfd391-4f70-4250-8000-801b33b9ae9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       image_name                                         image_path  class  \\\n",
      "0    Abyssinian_1    ./data/resized_images/trainval/Abyssinian_1.jpg    1.0   \n",
      "1   Abyssinian_10   ./data/resized_images/trainval/Abyssinian_10.jpg    1.0   \n",
      "2  Abyssinian_100  ./data/resized_images/trainval/Abyssinian_100.jpg    1.0   \n",
      "3  Abyssinian_101  ./data/resized_images/trainval/Abyssinian_101.jpg    1.0   \n",
      "4  Abyssinian_102  ./data/resized_images/trainval/Abyssinian_102.jpg    1.0   \n",
      "\n",
      "       fold  \n",
      "0  trainval  \n",
      "1  trainval  \n",
      "2  trainval  \n",
      "3  trainval  \n",
      "4  trainval  \n"
     ]
    }
   ],
   "source": [
    "# Load ground truth data\n",
    "pet_dataset_info = pd.read_csv(\"./data/pet_dataset_info.csv\")\n",
    "\n",
    "# Display first few rows\n",
    "print(pet_dataset_info.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccfd777c-a602-48c0-8e76-7cb2c7b53b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           image_name  predicted_class\n",
      "0    Abyssinian_2.jpg                0\n",
      "1   Abyssinian_20.jpg                0\n",
      "2  Abyssinian_201.jpg                0\n",
      "3  Abyssinian_202.jpg                0\n",
      "4  Abyssinian_204.jpg                0\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b6d94df-b8b2-4cec-b192-a61b53013885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                image_name  predicted_class  class  is_correct\n",
      "0             Abyssinian_2                1    1.0        True\n",
      "1            Abyssinian_20                1    1.0        True\n",
      "2           Abyssinian_201                1    1.0        True\n",
      "3           Abyssinian_202                1    1.0        True\n",
      "4           Abyssinian_204                1    1.0        True\n",
      "...                    ...              ...    ...         ...\n",
      "3664  yorkshire_terrier_95               37   37.0        True\n",
      "3665  yorkshire_terrier_96               37   37.0        True\n",
      "3666  yorkshire_terrier_97               37   37.0        True\n",
      "3667  yorkshire_terrier_98               37   37.0        True\n",
      "3668  yorkshire_terrier_99               31   37.0       False\n",
      "\n",
      "[3669 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Adjust the predicted_class by subtracting 1\n",
    "test_predictions[\"predicted_class\"] = test_predictions[\"predicted_class\"] + 1\n",
    "\n",
    "# Merge test predictions with ground truth\n",
    "test_predictions[\"image_name\"] = test_predictions[\"image_name\"].map(lambda x: x.replace(\".jpg\", \"\"))\n",
    "test_comparison = pd.merge(test_predictions, pet_dataset_info, on=\"image_name\", how=\"inner\")\n",
    "\n",
    "# Check if predictions match ground truth\n",
    "test_comparison[\"is_correct\"] = test_comparison[\"predicted_class\"] == test_comparison[\"class\"]\n",
    "\n",
    "# Display comparisons\n",
    "print(test_comparison[[\"image_name\", \"predicted_class\", \"class\", \"is_correct\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59d97429-3258-438d-8276-6c1a3ff74ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to ./data/test_predictions_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Save test predictions\n",
    "test_comparison.to_csv(\"./data/test_predictions_comparison.csv\", index=False)\n",
    "print(\"Test predictions saved to ./data/test_predictions_comparison.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6100cf6-9f97-484d-8f2e-bcef28a379b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.28%\n",
      "Test F1 Score (Weighted): 0.8821\n",
      "Test F1 Score (Macro): 0.8815\n",
      "Test metrics saved to output/test_metrics/test_metrics.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate accuracy for test data\n",
    "test_accuracy = test_comparison[\"is_correct\"].mean()\n",
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
    "\n",
    "# Calculate F1 scores for test data\n",
    "y_pred_test = test_comparison[\"predicted_class\"]\n",
    "y_true_test = test_comparison[\"class\"]\n",
    "\n",
    "f1_test_weighted = f1_score(y_true_test, y_pred_test, average=\"weighted\")\n",
    "f1_test_macro = f1_score(y_true_test, y_pred_test, average=\"macro\")\n",
    "\n",
    "print(f\"Test F1 Score (Weighted): {f1_test_weighted:.4f}\")\n",
    "print(f\"Test F1 Score (Macro): {f1_test_macro:.4f}\")\n",
    "\n",
    "# Save results to a dictionary\n",
    "test_metrics = {\n",
    "    \"test_accuracy\": test_accuracy,\n",
    "    \"f1_test_weighted\": f1_test_weighted,\n",
    "    \"f1_test_macro\": f1_test_macro\n",
    "}\n",
    "\n",
    "# Define the output directory and file\n",
    "output_dir = \"output/test_metrics\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "metrics_path = os.path.join(output_dir, \"test_metrics.json\")\n",
    "\n",
    "# Save the metrics to a JSON file\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=4)\n",
    "\n",
    "print(f\"Test metrics saved to {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414267ce-5530-4e1a-b429-07e6263a3ed2",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Start With Classifying with watermarks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "546f5283-2e27-448c-867e-132fc1f320bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(folder_path, model, transform, device):\n",
    "    predictions = []\n",
    "\n",
    "    for image_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "\n",
    "        # Skip non-image files\n",
    "        if not image_name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            continue\n",
    "\n",
    "        # Open and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_tensor)  # Get raw model output\n",
    "            if isinstance(outputs, torch.Tensor):  # Check if it's a PyTorch model\n",
    "                predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "            else:  # Assume it's a Hugging Face model output\n",
    "                predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "            predictions.append({\"image_name\": image_name, \"predicted_class\": predicted_class})\n",
    "\n",
    "    # Return predictions as a DataFrame\n",
    "    return pd.DataFrame(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c948138b-9f05-4175-8dce-3b308d8b64c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           image_name  predicted_class\n",
      "0    Abyssinian_2.jpg                0\n",
      "1   Abyssinian_20.jpg                0\n",
      "2  Abyssinian_201.jpg                0\n",
      "3        Bengal_1.jpg                5\n",
      "4       Bengal_19.jpg                5\n"
     ]
    }
   ],
   "source": [
    "#Classify Test Images\n",
    "\n",
    "# Path to test folder\n",
    "test_folder = \"./data/images_with_logos/test\"\n",
    "\n",
    "# Classify images in the test folder\n",
    "predictions_df = classify_images(test_folder, model, transform, device)\n",
    "\n",
    "# Display predictions\n",
    "print(predictions_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12b7bb67-39f1-4313-8c2b-e4eda69a4396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       image_name                                         image_path  class  \\\n",
      "0    Abyssinian_1    ./data/resized_images/trainval/Abyssinian_1.jpg    1.0   \n",
      "1   Abyssinian_10   ./data/resized_images/trainval/Abyssinian_10.jpg    1.0   \n",
      "2  Abyssinian_100  ./data/resized_images/trainval/Abyssinian_100.jpg    1.0   \n",
      "3  Abyssinian_101  ./data/resized_images/trainval/Abyssinian_101.jpg    1.0   \n",
      "4  Abyssinian_102  ./data/resized_images/trainval/Abyssinian_102.jpg    1.0   \n",
      "\n",
      "       fold  \n",
      "0  trainval  \n",
      "1  trainval  \n",
      "2  trainval  \n",
      "3  trainval  \n",
      "4  trainval  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load ground truth data\n",
    "pet_dataset_info = pd.read_csv(\"./data/pet_dataset_info.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(pet_dataset_info.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af336111-67bc-4c73-8ac5-e71d324b6a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                image_name  predicted_class  class  is_correct\n",
      "0             Abyssinian_2                1    1.0        True\n",
      "1            Abyssinian_20                1    1.0        True\n",
      "2           Abyssinian_201                1    1.0        True\n",
      "3                 Bengal_1                6    6.0        True\n",
      "4                Bengal_19                6    6.0        True\n",
      "...                    ...              ...    ...         ...\n",
      "3664      saint_bernard_99               29   29.0        True\n",
      "3665            samoyed_99               30   30.0        True\n",
      "3666          shiba_inu_99               32   32.0        True\n",
      "3667    wheaten_terrier_99               36   36.0        True\n",
      "3668  yorkshire_terrier_99               31   37.0       False\n",
      "\n",
      "[3669 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Compare Predictions with Ground Truth\n",
    "# Merge predictions with ground truth\n",
    "# Remove .jpg extension from image_name in predictions_df\n",
    "predictions_df[\"image_name\"] = predictions_df[\"image_name\"].map(lambda x: x.replace(\".jpg\", \"\"))\n",
    "\n",
    "comparison_df = pd.merge(predictions_df, pet_dataset_info, on=\"image_name\", how=\"inner\")\n",
    "\n",
    "\n",
    "comparison_df[\"predicted_class\"] = comparison_df[\"predicted_class\"] + 1\n",
    "# Add a comparison column\n",
    "comparison_df[\"is_correct\"] = comparison_df[\"predicted_class\"] == (comparison_df[\"class\"])  # Subtract 1 for 0-indexed classes\n",
    "\n",
    "# Display comparison\n",
    "print(comparison_df[[\"image_name\", \"predicted_class\", \"class\", \"is_correct\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f795275-b722-4913-b2cb-f315ac09b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.87%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = comparison_df[\"is_correct\"].mean()\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3776a3c9-5479-49cf-bb13-f2372a88f673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       image_name                                         image_path  class  \\\n",
      "0    Abyssinian_1    ./data/resized_images/trainval/Abyssinian_1.jpg    1.0   \n",
      "1   Abyssinian_10   ./data/resized_images/trainval/Abyssinian_10.jpg    1.0   \n",
      "2  Abyssinian_100  ./data/resized_images/trainval/Abyssinian_100.jpg    1.0   \n",
      "3  Abyssinian_101  ./data/resized_images/trainval/Abyssinian_101.jpg    1.0   \n",
      "4  Abyssinian_102  ./data/resized_images/trainval/Abyssinian_102.jpg    1.0   \n",
      "\n",
      "       fold  \n",
      "0  trainval  \n",
      "1  trainval  \n",
      "2  trainval  \n",
      "3  trainval  \n",
      "4  trainval  \n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "# Load the pet_dataset_info.csv\n",
    "pet_dataset_info = pd.read_csv(\"./data/pet_dataset_info.csv\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(pet_dataset_info.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a342c63e-b1b9-4d35-bf1c-8119d186b1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions comparison saved to ./data/predictions_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the comparison DataFrame to a CSV file\n",
    "comparison_df.to_csv(\"./data/predictions_comparison.csv\", index=False)\n",
    "\n",
    "print(\"Predictions comparison saved to ./data/predictions_comparison.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "728da273-1719-4c7d-ac6c-358df009376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.87%\n",
      "Test F1 Score (Weighted): 0.8074\n",
      "Test F1 Score (Macro): 0.8070\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import f1_score\n",
    "# Calculate accuracy\n",
    "accuracy = comparison_df[\"is_correct\"].mean()\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# For test data\n",
    "y_pred_test = comparison_df[\"predicted_class\"]\n",
    "y_true_test = comparison_df[\"class\"] \n",
    "\n",
    "f1_test_weighted = f1_score(y_true_test, y_pred_test, average=\"weighted\")\n",
    "f1_test_macro = f1_score(y_true_test, y_pred_test, average=\"macro\")\n",
    "\n",
    "print(f\"Test F1 Score (Weighted): {f1_test_weighted:.4f}\")\n",
    "print(f\"Test F1 Score (Macro): {f1_test_macro:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fae7db5-a229-42e7-bbec-12d3ec6401dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate accuracy for test data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcomparison_df\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_correct\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Calculate F1 scores for test data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comparison_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy for test data\n",
    "test_accuracy = comparison_df[\"is_correct\"].mean()\n",
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
    "\n",
    "# Calculate F1 scores for test data\n",
    "y_pred_test = comparison_df[\"predicted_class\"]\n",
    "y_true_test = comparison_df[\"class\"]\n",
    "\n",
    "f1_test_weighted = f1_score(y_true_test, y_pred_test, average=\"weighted\")\n",
    "f1_test_macro = f1_score(y_true_test, y_pred_test, average=\"macro\")\n",
    "\n",
    "print(f\"Test F1 Score (Weighted): {f1_test_weighted:.4f}\")\n",
    "print(f\"Test F1 Score (Macro): {f1_test_macro:.4f}\")\n",
    "\n",
    "# Save results to a dictionary\n",
    "test_metrics = {\n",
    "    \"prediction_accuracy\": test_accuracy,\n",
    "    \"f1_test_weighted\": f1_test_weighted,\n",
    "    \"f1_test_macro\": f1_test_macro\n",
    "}\n",
    "\n",
    "# Define the output directory and file\n",
    "output_der = \"output/prediction_metrics\"\n",
    "os.makedirs(output_der, exist_ok=True)\n",
    "metrics_path = os.path.join(output_der, \"prediction_metrics.json\")\n",
    "\n",
    "# Save the metrics to a JSON file\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=4)\n",
    "\n",
    "print(f\"Prediction metrics saved to {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53bd4c24-5333-4861-a59b-fdd97502666a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Display 5 random classified images with labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sampled_df \u001b[38;5;241m=\u001b[39m \u001b[43mcomparison_df\u001b[49m\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m sampled_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m      4\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Adjust folder as needed\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'comparison_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Display 5 random classified images with labels\n",
    "sampled_df = comparison_df.sample(5)\n",
    "for _, row in sampled_df.iterrows():\n",
    "    image_path = os.path.join(test_folder, f\"{row['image_name']}.jpg\")  # Adjust folder as needed\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Display the image inline\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Predicted: {row['predicted_class']}, Ground Truth: {row['class']}\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffc696a-f4c8-4875-94fa-3e425d77cafe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
